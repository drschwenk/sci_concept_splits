{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Table of Contents\n",
    "* [Load data](#Load-data)\n",
    "* [Hierarchical edge bundling](#Hierarchical-edge-bundling)\n",
    "\t* [computing similarity](#computing-similarity)\n",
    "\t* [clustering](#clustering)\n",
    "\t* [splitting](#splitting)\n",
    "\t* [rendering](#rendering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as st\n",
    "\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "heading_collapsed": true
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "dataset_root_dir = '/Users/schwenk/wrk/stb/dataset_releases/data_release_beta7/'\n",
    "file_name = 'tqa_dataset_beta7_5.json'\n",
    "data_file =  os.path.join(dataset_root_dir, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(dataset_root_dir, file_name), 'r') as f:\n",
    "    ck12_combined_dataset_raw = json.load(f)\n",
    "ck12_combined_dataset = deepcopy(ck12_combined_dataset_raw)\n",
    "\n",
    "with open('ck_12_vocab_words.pkl', 'rb') as f:\n",
    "    glossary_terms = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Hierarchical edge bundling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "heading_collapsed": true
   },
   "source": [
    "## computing similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "vocab_topics = ['Lesson Vocabulary', 'Vocabulary']\n",
    "cached_sw = stopwords.words('english') + list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def collect_filtered_lesson_text(complete_ds, include_adjunct=False, include_descriptions=False):\n",
    "    filtered_lesson_text = defaultdict(str)\n",
    "    lesson_names = {}\n",
    "    for lesson in complete_ds:\n",
    "        # lesson_key = lesson['lessonName'] + '_' + lesson['globalID']\n",
    "        lesson_key = lesson['globalID']\n",
    "        lesson_names[lesson_key] = lesson['lessonName']\n",
    "        for topic_name, topic in sorted(lesson['topics'].items(), key=lambda x: x[1]['globalID']):\n",
    "                    filtered_lesson_text[lesson_key] += topic['content']['text'] + '\\n'\n",
    "        if include_adjunct:\n",
    "            for topic_name, topic in lesson['adjunctTopics'].items():\n",
    "                if topic_name not in vocab_topics:\n",
    "                    filtered_lesson_text[lesson_key] += topic['content']['text'] + '\\n'\n",
    "        if include_descriptions:\n",
    "            for d_description in lesson['instructionalDiagrams'].values():\n",
    "                filtered_lesson_text[lesson_key] += d_description['processedText'] + '\\n'\n",
    "    return filtered_lesson_text, lesson_names\n",
    "\n",
    "def tokenize_and_stem(text, stopwords=cached_sw):\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    normalized_tokens = []\n",
    "    for toke in tokens:\n",
    "        cleaned_token = toke.strip().lower()\n",
    "        if len(cleaned_token) > 3 and cleaned_token not in stopwords and cleaned_token.isalpha():\n",
    "            stem = lemmatizer.lemmatize(cleaned_token)\n",
    "            normalized_tokens.append(stem)\n",
    "    return normalized_tokens\n",
    "\n",
    "def tokenize_and_mark_sci_terms(text, stopwords=cached_sw, science_terms=None):\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    normalized_tokens = []\n",
    "    for toke in tokens:\n",
    "        cleaned_token = toke.strip().lower()\n",
    "        if len(cleaned_token) > 3 and cleaned_token not in stopwords and cleaned_token.isalpha():\n",
    "            stem = lemmatizer.lemmatize(cleaned_token)\n",
    "            if stem in science_terms:\n",
    "                normalized_tokens.append('__CONCEPT__')\n",
    "            else:\n",
    "                normalized_tokens.append(stem)\n",
    "    return normalized_tokens\n",
    "\n",
    "def tokenize_lesson(text):\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    normalized_tokens = []\n",
    "    for toke in tokens:\n",
    "        cleaned_token = toke.strip().lower()\n",
    "        if cleaned_token and cleaned_token not in cached_sw and cleaned_token.isalpha():\n",
    "             normalized_tokens.append(cleaned_token)\n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "lesson_text, lesson_name_lookup = collect_filtered_lesson_text(ck12_combined_dataset, True, True)\n",
    "lesson_corp = [lesson for lid, lesson in sorted(lesson_text.items(), key=lambda x: x[0])]\n",
    "lesson_ids = [lid for lid, lesson in sorted(lesson_text.items(), key=lambda x: x[0])]\n",
    "lesson_names = [lesson_name_lookup[lid] for lid in lesson_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "tfizer = TfidfVectorizer(analyzer='word', tokenizer=lambda x: tokenize_and_stem(x, cached_sw), ngram_range=(1,3), min_df = 0.05, max_df=0.8)\n",
    "tfidf =  tfizer.fit_transform(lesson_corp)\n",
    "feature_names = tfizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pairwise_similarity = tfidf * tfidf.T\n",
    "pairwise_similar = pairwise_similarity.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "heading_collapsed": true
   },
   "source": [
    "## clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "num_clusters = 20\n",
    "km = KMeans(n_clusters=num_clusters, max_iter=10000 , n_init=10, n_jobs=7)\n",
    "km.fit(tfidf)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "cluster_members = defaultdict(list)\n",
    "for idx, cluster in enumerate(clusters):\n",
    "    cluster_members[cluster].append(lesson_ids[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "hidden": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 words:\n",
      " matter\n",
      " volume\n",
      " liquid\n",
      " change\n",
      " water\n",
      " solution\n",
      "\n",
      "\n",
      "Cluster 1 words:\n",
      " plate\n",
      " earthquake\n",
      " volcano\n",
      " ocean\n",
      " wind\n",
      " temperature\n",
      "\n",
      "\n",
      "Cluster 2 words:\n",
      " wave\n",
      " earthquake\n",
      " travel\n",
      " energy\n",
      " speed\n",
      " medium\n",
      "\n",
      "\n",
      "Cluster 3 words:\n",
      " blood\n",
      " system\n",
      " body\n",
      " disease\n",
      " cell\n",
      " organ\n",
      "\n",
      "\n",
      "Cluster 4 words:\n",
      " earth\n",
      " light\n",
      " planet\n",
      " moon\n",
      " star\n",
      " solar\n",
      "\n",
      "\n",
      "Cluster 5 words:\n",
      " acid\n",
      " protein\n",
      " molecule\n",
      " base\n",
      " cell\n",
      " sugar\n",
      "\n",
      "\n",
      "Cluster 6 words:\n",
      " nucleus\n",
      " nuclear\n",
      " radiation\n",
      " energy\n",
      " element\n",
      " atom\n",
      "\n",
      "\n",
      "Cluster 7 words:\n",
      " force\n",
      " object\n",
      " motion\n",
      " gravity\n",
      " distance\n",
      " greater\n",
      "\n",
      "\n",
      "Cluster 8 words:\n",
      " current\n",
      " electric\n",
      " magnetic\n",
      " field\n",
      " pole\n",
      " device\n",
      "\n",
      "\n",
      "Cluster 9 words:\n",
      " water\n",
      " soil\n",
      " pollution\n",
      " waste\n",
      " ocean\n",
      " river\n",
      "\n",
      "\n",
      "Cluster 10 words:\n",
      " science\n",
      " scientific\n",
      " scientist\n",
      " theory\n",
      " research\n",
      " experiment\n",
      "\n",
      "\n",
      "Cluster 11 words:\n",
      " energy\n",
      " heat\n",
      " transfer\n",
      " solar\n",
      " fuel\n",
      " matter\n",
      "\n",
      "\n",
      "Cluster 12 words:\n",
      " cell\n",
      " membrane\n",
      " organism\n",
      " nucleus\n",
      " bacteria\n",
      " reproduction\n",
      "\n",
      "\n",
      "Cluster 13 words:\n",
      " specie\n",
      " animal\n",
      " food\n",
      " organism\n",
      " population\n",
      " human\n",
      "\n",
      "\n",
      "Cluster 14 words:\n",
      " atom\n",
      " electron\n",
      " element\n",
      " bond\n",
      " compound\n",
      " metal\n",
      "\n",
      "\n",
      "Cluster 15 words:\n",
      " reaction\n",
      " chemical\n",
      " chemical reaction\n",
      " equation\n",
      " energy\n",
      " product\n",
      "\n",
      "\n",
      "Cluster 16 words:\n",
      " sound\n",
      " wave\n",
      " travel\n",
      " sense\n",
      " hair\n",
      " speed\n",
      "\n",
      "\n",
      "Cluster 17 words:\n",
      " rock\n",
      " mineral\n",
      " sediment\n",
      " layer\n",
      " form\n",
      " type\n",
      "\n",
      "\n",
      "Cluster 18 words:\n",
      " plant\n",
      " leaf\n",
      " factor\n",
      " trait\n",
      " tissue\n",
      " animal\n",
      "\n",
      "\n",
      "Cluster 19 words:\n",
      " fossil\n",
      " fuel\n",
      " carbon\n",
      " fossil fuel\n",
      " pollution\n",
      " coal\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster {} words:\".format(i))   \n",
    "    [print(' {}'.format(feature_names[ind])) for ind in top_centroids[i, :6]]\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def dict_key_extract(key, var):\n",
    "    if hasattr(var, 'items'):\n",
    "        for k, v in var.items():\n",
    "            if k == key:\n",
    "                yield v\n",
    "            if isinstance(v, dict):\n",
    "                for result in dict_key_extract(key, v):\n",
    "                    yield result\n",
    "            elif isinstance(v, list):\n",
    "                for d in v:\n",
    "                    for result in dict_key_extract(key, d):\n",
    "                        yield result\n",
    "\n",
    "def compute_split_stats(test_train_assignments):\n",
    "    stat_counts = {\n",
    "        'text_questions': {\n",
    "            'train': 0,\n",
    "            'test': 0,\n",
    "            'val': 0,\n",
    "            'id_to_find': 'nonDiagramQuestions'\n",
    "        },\n",
    "        'diagram_questions': {\n",
    "            'train': 0,\n",
    "            'test': 0,\n",
    "            'val': 0,\n",
    "            'id_to_find': 'diagramQuestions'\n",
    "        },\n",
    "        'topics': {\n",
    "            'train': 0,\n",
    "            'test': 0,\n",
    "            'val': 0,\n",
    "            'id_to_find': 'topics'\n",
    "        },\n",
    "    }\n",
    "    for split in ['test', 'train', 'val']:\n",
    "        for lesson_id in test_train_assignments[split]:\n",
    "            for stat_type, stats in stat_counts.items():\n",
    "                lesson_content = [lesson for lesson in ck12_combined_dataset if lesson['globalID'] == lesson_id][0]\n",
    "                stats[split] += len(list(dict_key_extract(stats['id_to_find'], lesson_content))[0].values())\n",
    "    stat_counts['n_lessons'] = {\n",
    "        \"test\": len(test_train_assignments['test']),\n",
    "        \"train\": len(test_train_assignments['train']),\n",
    "        \"val\": len(test_train_assignments['val']),\n",
    "        'id_to_find': 'n_lessons'\n",
    "    }\n",
    "    for stat_type, stat in stat_counts.items():\n",
    "        stat['train_fraction'] = \"{0:.3f}\".format(stat['train'] / (stat['train'] + stat['test'] + stat['val']))\n",
    "        stat['test_fraction'] = \"{0:.3f}\".format(stat['test'] / (stat['train'] + stat['test'] + stat['val']))\n",
    "        stat['val_fraction'] = \"{0:.3f}\".format(stat['val'] / (stat['train'] + stat['test'] + stat['val']))\n",
    "    return stat_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "manual_assignments = {\n",
    "                      \"covalent bonds\": \"train\",\n",
    "                      \"the sun and the earthmoon system\": \"train\",\n",
    "                      \"the senses\": \"train\",\n",
    "                      \"the nervous system\": \"train\",\n",
    "                      \"introduction to the solar system\": \"train\",\n",
    "                      \"nuclear energy\": \"train\",\n",
    "                      \"the digestive system\": \"train\",\n",
    "                      \"seasons\": \"train\",\n",
    "                      \"eclipses\": \"train\",\n",
    "    \n",
    "                      \"introduction to plants\": \"val\",\n",
    "                      \"inside earth\": \"val\",\n",
    "                      \"volcanic eruptions\": \"val\",\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('val', 310), ('train', 460), ('test', 306)]\n",
      "[('test', 0.2843866171003718), ('train', 0.4275092936802974), ('val', 0.28810408921933084)]\n"
     ]
    }
   ],
   "source": [
    "tt_assignments_id = defaultdict(list)\n",
    "for c, members in cluster_members.items():\n",
    "    member_names = [lesson_name_lookup[m] for m in members]\n",
    "    assigned_split = 'train'\n",
    "    rand_n = random.random()\n",
    "    man_assigned = set(member_names).intersection(set(manual_assignments.keys()))\n",
    "    if man_assigned:\n",
    "        man_assignments = [manual_assignments[ma] for ma in man_assigned]\n",
    "        assigned_split = manual_assignments[man_assigned.pop()]\n",
    "    elif rand_n > 0.8:\n",
    "        assigned_split = 'val'\n",
    "    elif rand_n > 0.55 :\n",
    "        assigned_split = 'test'\n",
    "    tt_assignments_id[assigned_split] += members\n",
    "\n",
    "tot_len = sum([len(v) for v in tt_assignments_id.values()])\n",
    "print([(k, len(v)) for k,v in tt_assignments_id.items()])\n",
    "print([(k, len(v) / tot_len) for k,v in sorted(tt_assignments_id.items())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# with open('./new_tt_assignments.json', 'w') as f:\n",
    "#     json.dump(tt_assignments_id, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "split_lookup ={}\n",
    "for k, vals in tt_assignments_id.items():\n",
    "    for v in vals:\n",
    "        split_lookup[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagram_questions</th>\n",
       "      <th>n_lessons</th>\n",
       "      <th>text_questions</th>\n",
       "      <th>topics</th>\n",
       "      <th>diagram_questions</th>\n",
       "      <th>n_lessons</th>\n",
       "      <th>text_questions</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train</th>\n",
       "      <td>0.715</td>\n",
       "      <td>0.704</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.693</td>\n",
       "      <td>8985</td>\n",
       "      <td>758</td>\n",
       "      <td>13036</td>\n",
       "      <td>3473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>0.157</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.171</td>\n",
       "      <td>1977</td>\n",
       "      <td>173</td>\n",
       "      <td>3279</td>\n",
       "      <td>859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test</th>\n",
       "      <td>0.128</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.136</td>\n",
       "      <td>1605</td>\n",
       "      <td>145</td>\n",
       "      <td>2614</td>\n",
       "      <td>683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       diagram_questions  n_lessons  text_questions  topics  \\\n",
       "train              0.715      0.704           0.689   0.693   \n",
       "val                0.157      0.161           0.173   0.171   \n",
       "test               0.128      0.135           0.138   0.136   \n",
       "\n",
       "       diagram_questions  n_lessons  text_questions  topics  \n",
       "train               8985        758           13036    3473  \n",
       "val                 1977        173            3279     859  \n",
       "test                1605        145            2614     683  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computed_stats_non_diagram = [compute_split_stats(tt_assignments_id)]\n",
    "\n",
    "split_trials_train_fracts = [{k:v['train_fraction'] for k, v in trial.items()} for trial in computed_stats_non_diagram]\n",
    "split_trials_train_counts = [{k:v['train'] for k, v in trial.items()} for trial in computed_stats_non_diagram]\n",
    "split_trials_test_fracts = [{k:v['test_fraction'] for k, v in trial.items()} for trial in computed_stats_non_diagram]\n",
    "split_trials_test_counts = [{k:v['test'] for k, v in trial.items()} for trial in computed_stats_non_diagram]\n",
    "split_trials_val_fracts = [{k:v['val_fraction'] for k, v in trial.items()} for trial in computed_stats_non_diagram]\n",
    "split_trials_val_counts = [{k:v['val'] for k, v in trial.items()} for trial in computed_stats_non_diagram]\n",
    "\n",
    "\n",
    "split_trial_df = pd.DataFrame(split_trials_train_fracts)\n",
    "split_trial_df = split_trial_df.append(pd.DataFrame(split_trials_val_fracts))\n",
    "split_trial_df = split_trial_df.append(pd.DataFrame(split_trials_test_fracts))\n",
    "\n",
    "split_trial_df.index = ['train', 'val', 'test']\n",
    "split_stats_df = pd.concat([split_trial_df, pd.DataFrame(split_trials_train_counts + split_trials_val_counts + split_trials_test_counts, index=['train', 'val', 'test'])], axis=1, join='inner')\n",
    "split_stats_df = split_stats_df.apply(pd.to_numeric)\n",
    "\n",
    "split_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "with open('./new_tt_assignments.json', 'r') as f:\n",
    "    tt_assignments_id = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_lookup ={}\n",
    "for k, vals in tt_assignments_id.items():\n",
    "    for v in vals:\n",
    "        split_lookup[v] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "all_lessons = [{'lname': lesson['lessonName'], 'tta': split_lookup[lesson['globalID']]} for lesson in ck12_combined_dataset if split_lookup[lesson['globalID']] != 'skip' if lesson['questions']['diagramQuestions'] or True]\n",
    "for lesson in all_lessons:\n",
    "    lesson['importName'] = lesson['tta'] + '.' + lesson['lname']\n",
    "\n",
    "connectivity_threshold = 0.80\n",
    "lesson_connections = []\n",
    "lessons_to_show = all_lessons\n",
    "\n",
    "affinity_to_use = pairwise_similar\n",
    "\n",
    "for i in range(len(lessons_to_show)):\n",
    "    this_lesson = lessons_to_show[i]\n",
    "    connected_lessons = []\n",
    "    connectivities = {}\n",
    "    for j in range(len(lessons_to_show)):\n",
    "        if i == j:\n",
    "            continue\n",
    "        if affinity_to_use[i][j] > connectivity_threshold and lessons_to_show[j]['importName'] != this_lesson['importName']:\n",
    "            connected_lessons.append(lessons_to_show[j]['importName'])\n",
    "    this_entry = {\n",
    "        'name': this_lesson['importName'].replace('vs.', 'vs'),\n",
    "        'imports': connected_lessons,\n",
    "        'size': '300',\n",
    "    }\n",
    "    if this_entry['imports']:\n",
    "        lesson_connections.append(this_entry)\n",
    "\n",
    "with open('lesson_connections.json', 'w') as f:\n",
    "    json.dump(lesson_connections, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"100%\" height=\"500\" src=\"index_ia.html?inline=false\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"100%\" height=\"500\" src=\"index_ia.html?inline=false\"></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
