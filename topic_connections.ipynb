{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Table of Contents\n",
    "* [Load data](#Load-data)\n",
    "* [Hierarchical edge bundling](#Hierarchical-edge-bundling)\n",
    "\t* [computing similarity](#computing-similarity)\n",
    "\t* [clustering](#clustering)\n",
    "\t* [splitting](#splitting)\n",
    "\t* [rendering](#rendering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as st\n",
    "\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "dataset_root_dir = '/Users/schwenk/wrk/stb/dataset_releases/data_release_beta7/'\n",
    "file_name = 'tqa_dataset_beta7_5.json'\n",
    "data_file =  os.path.join(dataset_root_dir, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(dataset_root_dir, file_name), 'r') as f:\n",
    "    ck12_combined_dataset_raw = json.load(f)\n",
    "ck12_combined_dataset = deepcopy(ck12_combined_dataset_raw)\n",
    "\n",
    "with open('ck_12_vocab_words.pkl', 'rb') as f:\n",
    "    glossary_terms = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Hierarchical edge bundling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## computing similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "vocab_topics = ['Lesson Vocabulary', 'Vocabulary']\n",
    "cached_sw = stopwords.words('english') + list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def collect_filtered_lesson_text(complete_ds, include_adjunct=False, include_descriptions=False):\n",
    "    filtered_lesson_text = defaultdict(str)\n",
    "    lesson_names = {}\n",
    "    for lesson in complete_ds:\n",
    "        # lesson_key = lesson['lessonName'] + '_' + lesson['globalID']\n",
    "        lesson_key = lesson['globalID']\n",
    "        lesson_names[lesson_key] = lesson['lessonName']\n",
    "        for topic_name, topic in sorted(lesson['topics'].items(), key=lambda x: x[1]['globalID']):\n",
    "                    filtered_lesson_text[lesson_key] += topic['content']['text'] + '\\n'\n",
    "        if include_adjunct:\n",
    "            for topic_name, topic in lesson['adjunctTopics'].items():\n",
    "                if topic_name not in vocab_topics:\n",
    "                    filtered_lesson_text[lesson_key] += topic['content']['text'] + '\\n'\n",
    "        if include_descriptions:\n",
    "            for d_description in lesson['instructionalDiagrams'].values():\n",
    "                filtered_lesson_text[lesson_key] += d_description['processedText'] + '\\n'\n",
    "    return filtered_lesson_text, lesson_names\n",
    "\n",
    "def tokenize_and_stem(text, stopwords=cached_sw):\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    normalized_tokens = []\n",
    "    for toke in tokens:\n",
    "        cleaned_token = toke.strip().lower()\n",
    "        if len(cleaned_token) > 3 and cleaned_token not in stopwords and cleaned_token.isalpha():\n",
    "            stem = lemmatizer.lemmatize(cleaned_token)\n",
    "            normalized_tokens.append(stem)\n",
    "    return normalized_tokens\n",
    "\n",
    "def tokenize_and_mark_sci_terms(text, stopwords=cached_sw, science_terms=None):\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    normalized_tokens = []\n",
    "    for toke in tokens:\n",
    "        cleaned_token = toke.strip().lower()\n",
    "        if len(cleaned_token) > 3 and cleaned_token not in stopwords and cleaned_token.isalpha():\n",
    "            stem = lemmatizer.lemmatize(cleaned_token)\n",
    "            if stem in science_terms:\n",
    "                normalized_tokens.append('__CONCEPT__')\n",
    "            else:\n",
    "                normalized_tokens.append(stem)\n",
    "    return normalized_tokens\n",
    "\n",
    "def tokenize_lesson(text):\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    normalized_tokens = []\n",
    "    for toke in tokens:\n",
    "        cleaned_token = toke.strip().lower()\n",
    "        if cleaned_token and cleaned_token not in cached_sw and cleaned_token.isalpha():\n",
    "             normalized_tokens.append(cleaned_token)\n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "lesson_text, lesson_name_lookup = collect_filtered_lesson_text(ck12_combined_dataset, True, True)\n",
    "lesson_corp = [lesson for lid, lesson in sorted(lesson_text.items(), key=lambda x: x[0])]\n",
    "lesson_ids = [lid for lid, lesson in sorted(lesson_text.items(), key=lambda x: x[0])]\n",
    "lesson_names = [lesson_name_lookup[lid] for lid in lesson_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "tfizer = TfidfVectorizer(analyzer='word', tokenizer=lambda x: tokenize_and_stem(x, cached_sw), ngram_range=(1,3), min_df = 0.05, max_df=0.8)\n",
    "tfidf =  tfizer.fit_transform(lesson_corp)\n",
    "feature_names = tfizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pairwise_similarity = tfidf * tfidf.T\n",
    "pairwise_similar = pairwise_similarity.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "num_clusters = 20\n",
    "km = KMeans(n_clusters=num_clusters, max_iter=10000 , n_init=10, n_jobs=7)\n",
    "km.fit(tfidf)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "cluster_members = defaultdict(list)\n",
    "for idx, cluster in enumerate(clusters):\n",
    "    cluster_members[cluster].append(lesson_ids[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 words:\n",
      " reaction\n",
      " chemical\n",
      " chemical reaction\n",
      " equation\n",
      " product\n",
      " energy\n",
      "\n",
      "\n",
      "Cluster 1 words:\n",
      " specie\n",
      " animal\n",
      " organism\n",
      " population\n",
      " food\n",
      " mammal\n",
      "\n",
      "\n",
      "Cluster 2 words:\n",
      " force\n",
      " object\n",
      " gravity\n",
      " motion\n",
      " greater\n",
      " work\n",
      "\n",
      "\n",
      "Cluster 3 words:\n",
      " electron\n",
      " atom\n",
      " element\n",
      " metal\n",
      " compound\n",
      " nucleus\n",
      "\n",
      "\n",
      "Cluster 4 words:\n",
      " mineral\n",
      " rock\n",
      " chemical\n",
      " water\n",
      " element\n",
      " surface\n",
      "\n",
      "\n",
      "Cluster 5 words:\n",
      " current\n",
      " electric\n",
      " magnetic\n",
      " field\n",
      " pole\n",
      " device\n",
      "\n",
      "\n",
      "Cluster 6 words:\n",
      " cell\n",
      " membrane\n",
      " organism\n",
      " body\n",
      " bacteria\n",
      " nucleus\n",
      "\n",
      "\n",
      "Cluster 7 words:\n",
      " earth\n",
      " planet\n",
      " moon\n",
      " orbit\n",
      " solar system\n",
      " solar\n",
      "\n",
      "\n",
      "Cluster 8 words:\n",
      " energy\n",
      " fuel\n",
      " heat\n",
      " nuclear\n",
      " fossil fuel\n",
      " power\n",
      "\n",
      "\n",
      "Cluster 9 words:\n",
      " wave\n",
      " sound\n",
      " travel\n",
      " speed\n",
      " energy\n",
      " medium\n",
      "\n",
      "\n",
      "Cluster 10 words:\n",
      " blood\n",
      " muscle\n",
      " system\n",
      " cell\n",
      " body\n",
      " heart\n",
      "\n",
      "\n",
      "Cluster 11 words:\n",
      " rock\n",
      " earthquake\n",
      " plate\n",
      " volcano\n",
      " crust\n",
      " sediment\n",
      "\n",
      "\n",
      "Cluster 12 words:\n",
      " acid\n",
      " protein\n",
      " base\n",
      " molecule\n",
      " compound\n",
      " cell\n",
      "\n",
      "\n",
      "Cluster 13 words:\n",
      " light\n",
      " matter\n",
      " pressure\n",
      " volume\n",
      " change\n",
      " object\n",
      "\n",
      "\n",
      "Cluster 14 words:\n",
      " system\n",
      " disease\n",
      " food\n",
      " body\n",
      " bacteria\n",
      " cancer\n",
      "\n",
      "\n",
      "Cluster 15 words:\n",
      " water\n",
      " soil\n",
      " pollution\n",
      " ocean\n",
      " zone\n",
      " river\n",
      "\n",
      "\n",
      "Cluster 16 words:\n",
      " science\n",
      " scientific\n",
      " star\n",
      " scientist\n",
      " theory\n",
      " research\n",
      "\n",
      "\n",
      "Cluster 17 words:\n",
      " carbon\n",
      " atom\n",
      " dioxide\n",
      " photosynthesis\n",
      " carbon dioxide\n",
      " pollution\n",
      "\n",
      "\n",
      "Cluster 18 words:\n",
      " wind\n",
      " temperature\n",
      " climate\n",
      " weather\n",
      " continent\n",
      " ocean\n",
      "\n",
      "\n",
      "Cluster 19 words:\n",
      " plant\n",
      " leaf\n",
      " animal\n",
      " factor\n",
      " trait\n",
      " like\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster {} words:\".format(i))   \n",
    "    [print(' {}'.format(feature_names[ind])) for ind in top_centroids[i, :6]]\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "manual_assignments = {\n",
    "                      \"covalent bonds\": \"train\",\n",
    "                      \"the sun and the earthmoon system\": \"train\",\n",
    "                      \"the senses\": \"train\",\n",
    "                      \"evolution and classification of plants\": \"test\",\n",
    "                      \"the nervous system\": \"train\",\n",
    "                      \"introduction to the solar system\": \"train\",\n",
    "                      \"nuclear energy\": \"train\",\n",
    "                      \"the digestive system\": \"train\",\n",
    "                      \"seasons\": \"train\",\n",
    "                      \"eclipses\": \"train\",\n",
    "\n",
    "    \n",
    "                      \"introduction to plants\": \"test\",\n",
    "                      \"inside earth\": \"test\",\n",
    "                      \"volcanic eruptions\": \"test\",\n",
    "#                       \"flow of energy\": \"test\",\n",
    "    \n",
    "#                       \"parts leaf\": \"val\", \n",
    "#                       \"cell structures\": \"val\",\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('train', 758), ('test', 175), ('val', 143)]\n",
      "[('test', 0.16263940520446096), ('train', 0.7044609665427509), ('val', 0.13289962825278812)]\n"
     ]
    }
   ],
   "source": [
    "tt_assignments_id = defaultdict(list)\n",
    "for c, members in cluster_members.items():\n",
    "    member_names = [lesson_name_lookup[m] for m in members]\n",
    "    assigned_split = 'train'\n",
    "    rand_n = random.random()\n",
    "    man_assigned = set(member_names).intersection(set(manual_assignments.keys()))\n",
    "    if man_assigned:\n",
    "        man_assignments = [manual_assignments[ma] for ma in man_assigned]\n",
    "        assigned_split = manual_assignments[man_assigned.pop()]\n",
    "    elif rand_n > 0.8:\n",
    "        assigned_split = 'test'\n",
    "    elif rand_n > 0.5 :\n",
    "        assigned_split = 'val'\n",
    "    tt_assignments_id[assigned_split] += members\n",
    "\n",
    "tot_len = sum([len(v) for v in tt_assignments_id.values()])\n",
    "print([(k, len(v)) for k,v in tt_assignments_id.items()])\n",
    "print([(k, len(v) / tot_len) for k,v in sorted(tt_assignments_id.items())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "with open('./new_tt_assignments.json', 'w') as f:\n",
    "    json.dump(tt_assignments_id, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "split_lookup ={}\n",
    "for k, vals in tt_assignments_id.items():\n",
    "    for v in vals:\n",
    "        split_lookup[v] = k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "all_lessons = [{'lname': lesson['lessonName'], 'tta': split_lookup[lesson['globalID']]} for lesson in ck12_combined_dataset if split_lookup[lesson['globalID']] != 'skip']\n",
    "for lesson in all_lessons:\n",
    "    lesson['importName'] = lesson['tta'] + '.' + lesson['lname']\n",
    "\n",
    "connectivity_threshold = 0.8\n",
    "lesson_connections = []\n",
    "lessons_to_show = all_lessons\n",
    "\n",
    "affinity_to_use = pairwise_similar\n",
    "\n",
    "for i in range(len(lessons_to_show)):\n",
    "    this_lesson = lessons_to_show[i]\n",
    "    connected_lessons = []\n",
    "    for j in range(len(lessons_to_show)):\n",
    "        if i == j:\n",
    "            continue\n",
    "        if affinity_to_use[i][j] > connectivity_threshold and lessons_to_show[j]['importName'] != this_lesson['importName']:\n",
    "            connected_lessons.append(lessons_to_show[j]['importName'])\n",
    "    this_entry = {\n",
    "        'name': this_lesson['importName'],\n",
    "        'imports': connected_lessons,\n",
    "        'size': '300'\n",
    "    }\n",
    "    if this_entry['imports']:\n",
    "        lesson_connections.append(this_entry)\n",
    "\n",
    "with open('lesson_connections.json', 'w') as f:\n",
    "    json.dump(lesson_connections, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"100%\" height=\"500\" src=\"index_ia.html?inline=false\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"100%\" height=\"500\" src=\"index_ia.html?inline=false\"></iframe>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
