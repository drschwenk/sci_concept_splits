{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Load data](#Load-data)\n",
    "* [Hierarchical edge bundling](#Hierarchical-edge-bundling)\n",
    "\t* [computing similarity](#computing-similarity)\n",
    "\t* [clustering](#clustering)\n",
    "\t* [splitting](#splitting)\n",
    "\t* [rendering](#rendering)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "dataset_root_dir = '/Users/schwenk/wrk/stb/dataset_releases/data_release_beta7/'\n",
    "file_name = 'tqa_dataset_beta7_5.json'\n",
    "data_file =  os.path.join(dataset_root_dir, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(dataset_root_dir, file_name), 'r') as f:\n",
    "    ck12_combined_dataset_raw = json.load(f)\n",
    "ck12_combined_dataset = deepcopy(ck12_combined_dataset_raw)\n",
    "\n",
    "with open('ck_12_vocab_words.pkl', 'rb') as f:\n",
    "    glossary_terms = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical edge bundling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## computing similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "vocab_topics = ['Lesson Vocabulary', 'Vocabulary']\n",
    "cached_sw = stopwords.words('english') + list(string.punctuation)\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def collect_filtered_lesson_text(complete_ds, include_adjunct=False, include_descriptions=False):\n",
    "    filtered_lesson_text = defaultdict(str)\n",
    "    lesson_names = {}\n",
    "    for lesson in complete_ds:\n",
    "        # lesson_key = lesson['lessonName'] + '_' + lesson['globalID']\n",
    "        lesson_key = lesson['globalID']\n",
    "        lesson_names[lesson_key] = lesson['lessonName']\n",
    "        for topic_name, topic in sorted(lesson['topics'].items(), key=lambda x: x[1]['globalID']):\n",
    "                    filtered_lesson_text[lesson_key] += topic['content']['text'] + '\\n'\n",
    "        if include_adjunct:\n",
    "            for topic_name, topic in lesson['adjunctTopics'].items():\n",
    "                if topic_name not in vocab_topics:\n",
    "                    filtered_lesson_text[lesson_key] += topic['content']['text'] + '\\n'\n",
    "        if include_descriptions:\n",
    "            for d_description in lesson['instructionalDiagrams'].values():\n",
    "                filtered_lesson_text[lesson_key] += d_description['processedText'] + '\\n'\n",
    "    return filtered_lesson_text, lesson_names\n",
    "\n",
    "def tokenize_and_stem(text, stopwords=cached_sw):\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    normalized_tokens = []\n",
    "    for toke in tokens:\n",
    "        cleaned_token = toke.strip().lower()\n",
    "        if len(cleaned_token) > 3 and cleaned_token not in stopwords and cleaned_token.isalpha():\n",
    "            stem = lemmatizer.lemmatize(cleaned_token)\n",
    "            normalized_tokens.append(stem)\n",
    "    return normalized_tokens\n",
    "\n",
    "def tokenize_and_mark_sci_terms(text, stopwords=cached_sw, science_terms=None):\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    normalized_tokens = []\n",
    "    for toke in tokens:\n",
    "        cleaned_token = toke.strip().lower()\n",
    "        if len(cleaned_token) > 3 and cleaned_token not in stopwords and cleaned_token.isalpha():\n",
    "            stem = lemmatizer.lemmatize(cleaned_token)\n",
    "            if stem in science_terms:\n",
    "                normalized_tokens.append('__CONCEPT__')\n",
    "            else:\n",
    "                normalized_tokens.append(stem)\n",
    "    return normalized_tokens\n",
    "\n",
    "def tokenize_lesson(text):\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    normalized_tokens = []\n",
    "    for toke in tokens:\n",
    "        cleaned_token = toke.strip().lower()\n",
    "        if cleaned_token and cleaned_token not in cached_sw and cleaned_token.isalpha():\n",
    "             normalized_tokens.append(cleaned_token)\n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "lesson_text, lesson_name_lookup = collect_filtered_lesson_text(ck12_combined_dataset, True, True)\n",
    "lesson_corp = [lesson for lid, lesson in sorted(lesson_text.items(), key=lambda x: x[0])]\n",
    "lesson_ids = [lid for lid, lesson in sorted(lesson_text.items(), key=lambda x: x[0])]\n",
    "lesson_names = [lesson_name_lookup[lid] for lid in lesson_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "tfizer = TfidfVectorizer(analyzer='word', tokenizer=lambda x: tokenize_and_stem(x, cached_sw), ngram_range=(1,3), min_df = 0.05, max_df=0.8)\n",
    "tfidf =  tfizer.fit_transform(lesson_corp)\n",
    "feature_names = tfizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "pairwise_similarity = tfidf * tfidf.T\n",
    "pairwise_similar = pairwise_similarity.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "num_clusters = 20\n",
    "km = KMeans(n_clusters=num_clusters, max_iter=10000 , n_init=10, n_jobs=7)\n",
    "km.fit(tfidf)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "cluster_members = defaultdict(list)\n",
    "for idx, cluster in enumerate(clusters):\n",
    "    cluster_members[cluster].append(lesson_ids[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 words:\n",
      " specie\n",
      " animal\n",
      " population\n",
      " mammal\n",
      " bird\n",
      " trait\n",
      "\n",
      "\n",
      "Cluster 1 words:\n",
      " reaction\n",
      " acid\n",
      " chemical\n",
      " molecule\n",
      " energy\n",
      " chemical reaction\n",
      "\n",
      "\n",
      "Cluster 2 words:\n",
      " earth\n",
      " planet\n",
      " moon\n",
      " orbit\n",
      " solar\n",
      " solar system\n",
      "\n",
      "\n",
      "Cluster 3 words:\n",
      " blood\n",
      " system\n",
      " body\n",
      " disease\n",
      " cell\n",
      " organ\n",
      "\n",
      "\n",
      "Cluster 4 words:\n",
      " science\n",
      " scientific\n",
      " soil\n",
      " scientist\n",
      " theory\n",
      " research\n",
      "\n",
      "\n",
      "Cluster 5 words:\n",
      " rock\n",
      " mineral\n",
      " sediment\n",
      " layer\n",
      " form\n",
      " surface\n",
      "\n",
      "\n",
      "Cluster 6 words:\n",
      " light\n",
      " star\n",
      " object\n",
      " matter\n",
      " distance\n",
      " speed\n",
      "\n",
      "\n",
      "Cluster 7 words:\n",
      " atom\n",
      " electron\n",
      " element\n",
      " bond\n",
      " compound\n",
      " metal\n",
      "\n",
      "\n",
      "Cluster 8 words:\n",
      " cell\n",
      " membrane\n",
      " organism\n",
      " nucleus\n",
      " protein\n",
      " bacteria\n",
      "\n",
      "\n",
      "Cluster 9 words:\n",
      " force\n",
      " object\n",
      " gravity\n",
      " motion\n",
      " greater\n",
      " work\n",
      "\n",
      "\n",
      "Cluster 10 words:\n",
      " energy\n",
      " heat\n",
      " matter\n",
      " transfer\n",
      " particle\n",
      " solar\n",
      "\n",
      "\n",
      "Cluster 11 words:\n",
      " fossil\n",
      " fuel\n",
      " carbon\n",
      " fossil fuel\n",
      " pollution\n",
      " coal\n",
      "\n",
      "\n",
      "Cluster 12 words:\n",
      " plant\n",
      " food\n",
      " bacteria\n",
      " organism\n",
      " insect\n",
      " ecosystem\n",
      "\n",
      "\n",
      "Cluster 13 words:\n",
      " wave\n",
      " sound\n",
      " travel\n",
      " speed\n",
      " energy\n",
      " medium\n",
      "\n",
      "\n",
      "Cluster 14 words:\n",
      " electric\n",
      " current\n",
      " device\n",
      " flow\n",
      " power\n",
      " energy\n",
      "\n",
      "\n",
      "Cluster 15 words:\n",
      " pressure\n",
      " wind\n",
      " weather\n",
      " temperature\n",
      " mass\n",
      " volume\n",
      "\n",
      "\n",
      "Cluster 16 words:\n",
      " plate\n",
      " continent\n",
      " crust\n",
      " ocean\n",
      " climate\n",
      " continental\n",
      "\n",
      "\n",
      "Cluster 17 words:\n",
      " water\n",
      " ocean\n",
      " liquid\n",
      " temperature\n",
      " solution\n",
      " dissolve\n",
      "\n",
      "\n",
      "Cluster 18 words:\n",
      " magnetic\n",
      " field\n",
      " pole\n",
      " north\n",
      " current\n",
      " earth\n",
      "\n",
      "\n",
      "Cluster 19 words:\n",
      " earthquake\n",
      " volcano\n",
      " plate\n",
      " wave\n",
      " volcanic\n",
      " building\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster {} words:\".format(i))   \n",
    "    [print(' {}'.format(feature_names[ind])) for ind in top_centroids[i, :6]]\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "manual_assignments = {\n",
    "                      \"insects and other arthropods\": \"train\",\n",
    "                      \"covalent bonds\": \"train\",\n",
    "                      \"the sun and the earthmoon system\": \"train\",\n",
    "                      \"the senses\": \"train\",\n",
    "                      \"the respiratory system\": \"test\",\n",
    "                      \"evolution and classification of plants\": \"test\",\n",
    "                      \"the nervous system\": \"train\",\n",
    "                      \"parts leaf\": \"train\",\n",
    "                      \"volcanic eruptions\": \"train\",\n",
    "                      \"introduction to the solar system\": \"train\",\n",
    "                      \"nuclear energy\": \"train\",\n",
    "    \n",
    "                      \"seasons\": \"test\",\n",
    "                      \"introduction to plants\": \"test\",\n",
    "                      \"cell structures\": \"train\",\n",
    "                      \"inside the atom\": \"test\",\n",
    "                      \"inside earth\": \"test\",\n",
    "                      \"vision and the eye\": \"test\",\n",
    "                      \"the digestive system\": \"train\",\n",
    "                      \"eclipses\": \"test\",\n",
    "                      \"flow of energy\": \"test\"\n",
    "                     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287 789\n",
      "0.266728624535316 0.733271375464684\n"
     ]
    }
   ],
   "source": [
    "tt_assignments_id = defaultdict(list)\n",
    "for c, members in cluster_members.items():\n",
    "    member_names = [lesson_name_lookup[m] for m in members]\n",
    "    assigned_split = 'train'\n",
    "    rand_n = random.random()\n",
    "    man_assigned = set(member_names).intersection(set(manual_assignments.keys()))\n",
    "    if man_assigned:\n",
    "        assigned_split = manual_assignments[man_assigned.pop()]\n",
    "    elif rand_n > 0.95:\n",
    "        assigned_split = 'test'\n",
    "    tt_assignments_id[assigned_split] += members\n",
    "\n",
    "tot_len = len(tt_assignments_id['test']) + len(tt_assignments_id['train'])\n",
    "print(len(tt_assignments_id['test']), len(tt_assignments_id['train']))\n",
    "print(len(tt_assignments_id['test']) / tot_len, len(tt_assignments_id['train'])/ tot_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "with open('./new_tt_assignments.json', 'w') as f:\n",
    "    json.dump(tt_assignments_id, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "split_lookup ={}\n",
    "for k, vals in tt_assignments_id.items():\n",
    "    for v in vals:\n",
    "        split_lookup[v] = k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "all_lessons = [{'lname': lesson['lessonName'], 'tta': split_lookup[lesson['globalID']]} for lesson in ck12_combined_dataset if split_lookup[lesson['globalID']] != 'skip']\n",
    "for lesson in all_lessons:\n",
    "    lesson['importName'] = lesson['tta'] + '.' + lesson['lname']\n",
    "\n",
    "connectivity_threshold = 0.8\n",
    "lesson_connections = []\n",
    "lessons_to_show = all_lessons\n",
    "\n",
    "affinity_to_use = pairwise_similar\n",
    "\n",
    "for i in range(len(lessons_to_show)):\n",
    "    this_lesson = lessons_to_show[i]\n",
    "    connected_lessons = []\n",
    "    for j in range(len(lessons_to_show)):\n",
    "        if i == j:\n",
    "            continue\n",
    "        if affinity_to_use[i][j] > connectivity_threshold and lessons_to_show[j]['importName'] != this_lesson['importName']:\n",
    "            connected_lessons.append(lessons_to_show[j]['importName'])\n",
    "    this_entry = {\n",
    "        'name': this_lesson['importName'],\n",
    "        'imports': connected_lessons,\n",
    "        'size': '300'\n",
    "    }\n",
    "    if this_entry['imports']:\n",
    "        lesson_connections.append(this_entry)\n",
    "\n",
    "with open('lesson_connections.json', 'w') as f:\n",
    "    json.dump(lesson_connections, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"100%\" height=\"500\" src=\"index_ia.html?inline=false\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"100%\" height=\"500\" src=\"index_ia.html?inline=false\"></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
